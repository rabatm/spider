# üï∑Ô∏è Arachnida - Un Scraper Web en Rust

Arachnida est un projet r√©alis√© dans le cadre du cursus de l'√©cole 42, ax√© sur le web scraping et l'analyse de m√©tadonn√©es. Ce projet a √©t√© enti√®rement d√©velopp√© en **Rust** en suivant les principes de la **Clean Architecture** pour garantir un code modulaire, testable et maintenable.

Il se compose de deux programmes distincts :
* `spider` : Un outil en ligne de commande pour t√©l√©charger r√©cursivement les images d'un site web.
* `scorpion` : Un outil pour extraire et afficher les m√©tadonn√©es EXIF des images.

---

## üßÖ Architecture

Ce projet a √©t√© con√ßu en suivant les principes de la **Clean Architecture** pour s√©parer clairement les responsabilit√©s du programme. L'organisation du code est divis√©e en trois couches principales :

* **üíõ Domain** : Le c≈ìur de l'application. Contient les entit√©s m√©tier pures (ex: la structure `Image`) sans aucune d√©pendance technique.
* **üß† Application** : La couche logique. D√©finit les "contrats" (`traits`) qui d√©crivent ce que l'application doit faire, et les "Cas d'Usage" (Use Cases) qui orchestrent la logique m√©tier.
* **üåç Infrastructure** : La couche externe. Contient les impl√©mentations concr√®tes des contrats en utilisant des outils techniques (ex: `reqwest` pour les requ√™tes HTTP, `scraper` pour le parsing HTML, `std::fs` pour l'√©criture de fichiers).

Le choix d'impl√©menter la Clean Architecture pour ce projet, bien qu'il s'agisse d'un simple outil en ligne de commande, est une d√©marche d√©lib√©r√©e visant √† mettre en pratique des principes d'ing√©nierie logicielle robustes.

L'objectif √©tait d'aller au-del√† d'un simple script fonctionnel pour construire une application dont la structure offre plusieurs avantages cl√©s :

* **Testabilit√© Accrue** : La logique m√©tier (les "Cas d'Usage") est compl√®tement d√©coupl√©e des d√©tails techniques. Cela signifie qu'on peut la tester de mani√®re unitaire sans avoir besoin d'un vrai r√©seau ou d'un vrai syst√®me de fichiers, en utilisant des "mocks".

* **Flexibilit√© et √âvolution** : L'application est "pluggable". Aujourd'hui, elle sauvegarde les images sur le disque local. Demain, on pourrait facilement ajouter une nouvelle impl√©mentation pour sauvegarder sur un service cloud (comme Amazon S3) sans changer une seule ligne de la logique m√©tier.

* **Ind√©pendance aux Frameworks** : Les biblioth√®ques comme `reqwest` ou `scraper` sont trait√©es comme des d√©tails d'impl√©mentation, et non comme le c≈ìur de l'application. Le "m√©tier" ne d√©pend pas d'elles, ce sont elles qui s'adaptent au "m√©tier".

* **Compr√©hensibilit√©** : La s√©paration claire des couches rend le code plus facile √† lire, √† comprendre et √† maintenir, pour moi-m√™me ou pour un autre d√©veloppeur (ou un correcteur !).

En somme, ce projet √©tait autant un exercice sur l'√©criture de code qui **fonctionne** qu'un exercice sur l'√©criture de code **durable** et **bien con√ßu**.

---

## ‚ú® Fonctionnalit√©s

### Spider
* T√©l√©chargement r√©cursif des images d'un site √† partir d'une URL de d√©part.
* Profondeur de r√©cursion configurable avec l'option `-l` (par d√©faut : 5).
* Chemin de sauvegarde des images configurable avec l'option `-p` (par d√©faut : `./data/`).
* Filtrage des images par extensions : `.jpg`, `.jpeg`, `.png`, `.gif`, `.bmp`.

### Scorpion
* Extraction et affichage des m√©tadonn√©es (EXIF) d'un ou plusieurs fichiers image.
* Compatible avec les m√™mes formats d'image que le `spider`.

---

## üõ†Ô∏è Installation et Compilation

Le projet utilise **Cargo**, le gestionnaire de paquets et outil de build de Rust. Un `Makefile` est √©galement fourni pour simplifier les commandes courantes, conform√©ment aux standards de 42.

1.  **Clonez le d√©p√¥t :**
    ```bash
    git clone [https://github.com/VOTRE_LOGIN/arachnida.git](https://github.com/VOTRE_LOGIN/arachnida.git)
    cd arachnida
    ```

2.  **Compilez le projet :**
    La mani√®re la plus simple est d'utiliser le `Makefile`. Cette commande va compiler le projet en mode "release" (optimis√©) et copier le binaire √† la racine.
    ```bash
    make
    ```

---

## üöÄ Utilisation

### Spider
Le programme prend une URL en param√®tre ainsi que des options.

```bash
./spider [-r] [-l PROFONDEUR] [-p CHEMIN] URL